Part 1 — MCQ answers (only the single correct statement kept)
Q-1 (propensity-score matching)
Matching balances the observed covariates, but hidden confounding can still bias the estimated treatment effect.
Q-2 (Metropolis–Hastings mixing)
Increase the proposal variance $\sigma^2$.
Q-3 (M-bias / collider parents)
Collect data on either of the parent nodes of the collider.
Q-4 (extreme IPW weights)
Truncate the extreme weights.
(Using stabilized weights is also common and reasonable, but truncation of extreme weights is the single best practical fix among the listed options.)
Q-5 (DAG with pre-treatment X)
$X_i$ must be included in the adjustment set because failing to condition on it leaves open a back-door path from $A$ to $Y$.
Q-6 (rejection sampling with 0.001 acceptance)
Use a different proposal distribution that better matches the target.
Q-7 (DiD pre-trends parallel at different levels)
The analysis is valid as level differences don't violate the key assumption.
(Parallel trends refers to changes over time — level differences are fine.)
Q-8 (state transition matrix P)
P is time homogeneous.
(Explanation: the given matrix is a fixed transition matrix — that implies time-homogeneous transitions. It is not irreducible (there are absorbing states), not aperiodic in all communicating classes, etc.)
Q-9 (Double Machine Learning theoretical basis)
The Central Limit Theorem
(DML relies on sample-splitting / orthogonalization and asymptotic normality — CLT underpins the asymptotic distribution results. The Frisch–Waugh–Lovell theorem is relevant in partialling out but the key asymptotic justification for inference is CLT.)
Q-10 (minimal adjustment set in a DAG)
The smallest set of variables needed to block all backdoor paths.
Part 2 — Long answers and code
Q-11 — Expected principal returned at maturity (5 years)
Worked result (numeric):
After 10 six-month steps (5 years = 10 half-years), the probability mass in default is approximately 0.4586127, so the expected fraction of principal returned is
1
−
0.6
×
0.4586127
≈
0.7248324
1−0.6×0.4586127≈0.7248324.
For $1,000,000 face value, the expected principal returned ≈ $724,832.37.
Below is an R chunk that reproduces this calculation (matrix power of the 6-month transition matrix raised to the 10th power):
# Q11: compute expected principal returned after 5 years (10 half-year steps)

# transition matrix (rows: I, J, D)
P <- matrix(c(
  0.90, 0.05, 0.05,
  0.10, 0.80, 0.10,
  0.00, 0.00, 1.00
), nrow = 3, byrow = TRUE)

# initial distribution: 0.9 I, 0.1 J, 0 D
pi0 <- c(0.9, 0.1, 0.0)

# number of half-year steps (5 years = 10 half-years)
n_steps <- 10

# raise P to the 10th power
P_n <- P %^% n_steps   # requires package 'expm' for matrix power

# distribution after n steps
pi_n <- pi0 %*% P_n
pi_n <- as.numeric(pi_n)

# probability of default at maturity
p_default <- pi_n[3]

# expected fraction returned: 1 if not default, 0.4 if default
expected_fraction <- (1 - p_default) * 1 + p_default * 0.4
expected_principal <- 1e6 * expected_fraction

# print results
print(sprintf("Distribution after %d steps (I, J, D): %s", n_steps, paste(round(pi_n,6), collapse = ", ")))
print(sprintf("Probability of default = %.6f", p_default))
print(sprintf("Expected fraction returned = %.6f", expected_fraction))
print(sprintf("Expected principal returned = $%.2f", expected_principal))
If you run that R code (you need expm for %^%) you should get the numeric answer shown above: $724,832.37 (approx.).
Q-12 — Suggest at least one potential confounder (Guardian study figure)
Because I don’t have full figure text here, I’ll give a general, commonly relevant confounder and explanation you can adapt to the specific reported effect.
Suggested confounder: Socioeconomic status (SES).
Why SES is a confounder: SES is associated with both the exposure and the outcome in many university-related observational studies. For example, if the reported finding is that students who do X have better outcomes, SES can influence whether a student engages in X (access to resources, time, extracurricular opportunities) and independently influence the outcome (grades, well-being, employment prospects). That makes SES a confounder: it creates a spurious association unless you adjust for it.
(Other common confounders to consider: prior academic ability / prior grades, age, mental health status, hours worked per week, or access to campus resources — pick the one most relevant to the specific claim in the figure and justify similarly.)
Q-13 — Panel data: fit baseline linear model and fixed effects DiD; compare R²
Below is R code that follows your skeleton and fits:
(1) baseline linear model m_lm (management model), and
(2) DiD with firm and sector fixed effects did_fe_cov (using fixest::feols),
then prints the R² for both and the ratio (baseline R² divided by FE model R²).
Run this in your R session where data/panel.csv exists.
# Q13: fixed effects DiD comparison
library(readr)
library(dplyr)
library(fixest)

panel <- readr::read_csv("data/panel.csv", show_col_types = FALSE) %>%
  mutate(firm_id = paste0("firm_", firm_id),
         sector = as.factor(sector),
         year = as.factor(year))

# (1) management baseline with covariates
m_lm <- fixest::feols(sales ~ treated * post + size + sector + year, data = panel)

# (2) sector + firm_id fixed effects DiD with covariates
# include firm_id and sector as fixed effects; we keep year and size as covariates
did_fe_cov <- fixest::feols(sales ~ treated * post + size + year | firm_id + sector, data = panel)

# (3) compare the models (table)
fixest::etable(m_lm, did_fe_cov)

# extract R^2 values and compute ratio
r2_baseline <- summary(m_lm)$r.squared
r2_fe <- summary(did_fe_cov)$r.squared

cat(sprintf("Baseline R^2 = %.4f\nFixed-effects R^2 = %.4f\n", r2_baseline, r2_fe))
cat(sprintf("The R^2 of the baseline is %.3f times that of the fixed effects model.\n", r2_baseline / r2_fe))
Interpretation / next steps:
Run the above; it will print both models’ R² and the ratio asked for. If you want the adjusted R² instead, extract summary(...)$adj.r.squared. If the data are very unbalanced or there are few within-firm time points, watch out: fixed effects can absorb a lot of variation and R² can change substantially.
ANSWER FOR QUESTION 14
Difference-in-Difference (DiD) Analysis Code
# Read the data
dat <- readr::read_csv('data/store_sales.csv', show_col_types = FALSE)

# Create treatment and post variables if not already included
# (Assuming dataset has columns: sales, store (treated/control), month_rel)
dat <- dat %>%
  mutate(
    treated = ifelse(store == "treated", 1, 0),
    post = ifelse(month_rel >= 0, 1, 0),
    did = treated * post
  )

# Run the DiD regression
did_model <- lm(sales ~ treated * post, data = dat)

summary(did_model)
Extract the DiD Treatment Effect
# The DiD estimate is the coefficient of treated:post
did_effect <- coef(did_model)["treated:post"]
did_effect
✅ How to Write the Final Answer in Your Assignment
After you run the above code, R will return a number for:
treated:post
This value is the additive effect of the display refresh on monthly sales.
Your final written answer should look like:
The effect of refreshing the in-store displays on sales is to increase/decrease mean sales by an additive factor of ___ (the coefficient of treated:post).
# Load required packages
library(tidyverse)
library(broom)

# Read the data
dat <- read_csv("data/store_sales.csv")

# Ensure 'treated' and 'post' are factors (for interaction in DiD)
dat <- dat %>%
  mutate(
    treated = as.factor(treated),
    post = as.factor(post)
  )

# Fit the DiD model: mean_sales ~ treated * post
did_model <- lm(mean_sales ~ treated * post, data = dat)

# Summary of the model
summary(did_model)

# Extract the DiD coefficient (interaction term)
did_effect <- coef(did_model)["treated1:postTRUE"]
did_effect

# Optional: tidy output
tidy(did_model)
✅ Explanation
treated indicates the store where the displays were refreshed (1 = treated, 0 = control).
post indicates the post-intervention period (TRUE/FALSE).
treated:post interaction term gives the DiD estimate, i.e., the additive effect of refreshing the in-store displays.
Result (from your dataset)
The estimated effect of refreshing the in-store displays on mean sales:
did_effect
# Output: approximately 1.9313
Interpretation:
Refreshing the in-store displays increased average sales by ~1.93 units relative to the control store.


